coder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.23.self_attn.v_proj.base_layer.bias
[2025-03-13 15:11:05,586][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.bias
[2025-03-13 15:11:05,586][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.bias
[2025-03-13 15:11:05,586][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.23.self_attn.o_proj.bias
[2025-03-13 15:11:05,586][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.23.mlp.gate_proj.bias
[2025-03-13 15:11:05,586][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.23.mlp.up_proj.bias
[2025-03-13 15:11:05,586][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.23.mlp.down_proj.bias
[2025-03-13 15:11:05,586][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.24.self_attn.q_proj.base_layer.bias
[2025-03-13 15:11:05,586][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.bias
[2025-03-13 15:11:05,586][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.bias
[2025-03-13 15:11:05,586][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.24.self_attn.k_proj.base_layer.bias
[2025-03-13 15:11:05,586][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.bias
[2025-03-13 15:11:05,586][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.bias
[2025-03-13 15:11:05,587][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.24.self_attn.v_proj.base_layer.bias
[2025-03-13 15:11:05,587][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.bias
[2025-03-13 15:11:05,587][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.bias
[2025-03-13 15:11:05,587][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.24.self_attn.o_proj.bias
[2025-03-13 15:11:05,587][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.24.mlp.gate_proj.bias
[2025-03-13 15:11:05,587][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.24.mlp.up_proj.bias
[2025-03-13 15:11:05,587][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.24.mlp.down_proj.bias
[2025-03-13 15:11:05,587][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.25.self_attn.q_proj.base_layer.bias
[2025-03-13 15:11:05,587][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.bias
[2025-03-13 15:11:05,587][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.bias
[2025-03-13 15:11:05,587][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.25.self_attn.k_proj.base_layer.bias
[2025-03-13 15:11:05,587][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.bias
[2025-03-13 15:11:05,587][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.bias
[2025-03-13 15:11:05,587][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.25.self_attn.v_proj.base_layer.bias
[2025-03-13 15:11:05,587][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.bias
[2025-03-13 15:11:05,587][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.bias
[2025-03-13 15:11:05,588][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.25.self_attn.o_proj.bias
[2025-03-13 15:11:05,588][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.25.mlp.gate_proj.bias
[2025-03-13 15:11:05,588][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.25.mlp.up_proj.bias
[2025-03-13 15:11:05,588][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.25.mlp.down_proj.bias
[2025-03-13 15:11:05,588][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.26.self_attn.q_proj.base_layer.bias
[2025-03-13 15:11:05,588][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.bias
[2025-03-13 15:11:05,588][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.bias
[2025-03-13 15:11:05,588][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.26.self_attn.k_proj.base_layer.bias
[2025-03-13 15:11:05,588][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.bias
[2025-03-13 15:11:05,588][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.bias
[2025-03-13 15:11:05,588][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.26.self_attn.v_proj.base_layer.bias
[2025-03-13 15:11:05,588][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.bias
[2025-03-13 15:11:05,588][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.bias
[2025-03-13 15:11:05,588][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.26.self_attn.o_proj.bias
[2025-03-13 15:11:05,588][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.26.mlp.gate_proj.bias
[2025-03-13 15:11:05,589][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.26.mlp.up_proj.bias
[2025-03-13 15:11:05,589][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.26.mlp.down_proj.bias
[2025-03-13 15:11:05,589][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.27.self_attn.q_proj.base_layer.bias
[2025-03-13 15:11:05,589][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.bias
[2025-03-13 15:11:05,589][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.bias
[2025-03-13 15:11:05,589][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.27.self_attn.k_proj.base_layer.bias
[2025-03-13 15:11:05,589][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.bias
[2025-03-13 15:11:05,589][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.bias
[2025-03-13 15:11:05,589][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.27.self_attn.v_proj.base_layer.bias
[2025-03-13 15:11:05,589][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.bias
[2025-03-13 15:11:05,589][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.bias
[2025-03-13 15:11:05,589][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.27.self_attn.o_proj.bias
[2025-03-13 15:11:05,589][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.27.mlp.gate_proj.bias
[2025-03-13 15:11:05,589][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.27.mlp.up_proj.bias
[2025-03-13 15:11:05,589][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.27.mlp.down_proj.bias
[2025-03-13 15:11:05,589][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.28.self_attn.q_proj.base_layer.bias
[2025-03-13 15:11:05,590][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.bias
[2025-03-13 15:11:05,590][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.bias
[2025-03-13 15:11:05,590][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.28.self_attn.k_proj.base_layer.bias
[2025-03-13 15:11:05,590][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.bias
[2025-03-13 15:11:05,590][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.bias
[2025-03-13 15:11:05,590][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.28.self_attn.v_proj.base_layer.bias
[2025-03-13 15:11:05,590][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.bias
[2025-03-13 15:11:05,590][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.bias
[2025-03-13 15:11:05,590][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.28.self_attn.o_proj.bias
[2025-03-13 15:11:05,590][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.28.mlp.gate_proj.bias
[2025-03-13 15:11:05,590][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.28.mlp.up_proj.bias
[2025-03-13 15:11:05,590][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.28.mlp.down_proj.bias
[2025-03-13 15:11:05,590][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.29.self_attn.q_proj.base_layer.bias
[2025-03-13 15:11:05,590][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.bias
[2025-03-13 15:11:05,590][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.bias
[2025-03-13 15:11:05,590][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.29.self_attn.k_proj.base_layer.bias
[2025-03-13 15:11:05,591][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.bias
[2025-03-13 15:11:05,591][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.bias
[2025-03-13 15:11:05,591][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.29.self_attn.v_proj.base_layer.bias
[2025-03-13 15:11:05,591][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.bias
[2025-03-13 15:11:05,591][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.bias
[2025-03-13 15:11:05,591][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.29.self_attn.o_proj.bias
[2025-03-13 15:11:05,591][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.29.mlp.gate_proj.bias
[2025-03-13 15:11:05,591][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.29.mlp.up_proj.bias
[2025-03-13 15:11:05,591][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.29.mlp.down_proj.bias
[2025-03-13 15:11:05,591][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.30.self_attn.q_proj.base_layer.bias
[2025-03-13 15:11:05,591][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.bias
[2025-03-13 15:11:05,591][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.bias
[2025-03-13 15:11:05,591][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.30.self_attn.k_proj.base_layer.bias
[2025-03-13 15:11:05,591][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.bias
[2025-03-13 15:11:05,591][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.bias
[2025-03-13 15:11:05,591][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.30.self_attn.v_proj.base_layer.bias
[2025-03-13 15:11:05,591][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.bias
[2025-03-13 15:11:05,592][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.bias
[2025-03-13 15:11:05,592][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.30.self_attn.o_proj.bias
[2025-03-13 15:11:05,592][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.30.mlp.gate_proj.bias
[2025-03-13 15:11:05,592][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.30.mlp.up_proj.bias
[2025-03-13 15:11:05,592][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.30.mlp.down_proj.bias
[2025-03-13 15:11:05,592][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.31.self_attn.q_proj.base_layer.bias
[2025-03-13 15:11:05,592][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.bias
[2025-03-13 15:11:05,592][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.bias
[2025-03-13 15:11:05,592][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.31.self_attn.k_proj.base_layer.bias
[2025-03-13 15:11:05,592][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.bias
[2025-03-13 15:11:05,592][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.bias
[2025-03-13 15:11:05,592][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.31.self_attn.v_proj.base_layer.bias
[2025-03-13 15:11:05,592][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.bias
[2025-03-13 15:11:05,592][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.bias
[2025-03-13 15:11:05,592][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.31.self_attn.o_proj.bias
[2025-03-13 15:11:05,592][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.31.mlp.gate_proj.bias
[2025-03-13 15:11:05,593][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.31.mlp.up_proj.bias
[2025-03-13 15:11:05,593][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.model.layers.31.mlp.down_proj.bias
[2025-03-13 15:11:05,593][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor_video.resnet.frontend3D.0.bias <- decoder.base_model.model.lm_head.bias
[2025-03-13 15:11:05,593][fairseq.utils][INFO] - ***********************CUDA enviroments for all 1 workers***********************
[2025-03-13 15:11:05,593][fairseq.utils][INFO] - rank   0: capabilities =  7.0  ; total memory = 31.739 GB ; name = Tesla V100-PCIE-32GB                    
[2025-03-13 15:11:05,594][fairseq.utils][INFO] - ***********************CUDA enviroments for all 1 workers***********************
[2025-03-13 15:11:05,594][fairseq_cli.train][INFO] - training on 1 devices (GPUs/TPUs)
[2025-03-13 15:11:05,594][fairseq_cli.train][INFO] - max tokens per device = None and max sentences per device = 1
[2025-03-13 15:11:05,595][fairseq.trainer][INFO] - Preparing to load checkpoint checkpoints/checkpoint_last.pt
[2025-03-13 15:11:05,595][fairseq.trainer][INFO] - No existing checkpoint found checkpoints/checkpoint_last.pt
[2025-03-13 15:11:05,595][fairseq.trainer][INFO] - loading train data for epoch 1
[2025-03-13 15:11:05,595][src.vsp_llm_training][INFO] - Using tokenizer
MANIFEST_PATH /home/jupyter-samantha_caasi@dls-bf571/datasets/datasets_for_vsp-llm/lrs3/muavic_dataset/train.tsv
[2025-03-13 15:11:05,692][src.vsp_llm_dataset][INFO] - max_keep=500, min_keep=None, loaded 30782, skipped 0 short and 0 long and 0 unaligned, longest-loaded=155, shortest-loaded=12
[2025-03-13 15:11:05,869][src.vsp_llm_dataset][INFO] - /home/jupyter-samantha_caasi@dls-bf571/datasets/datasets_for_vsp-llm/lrs3/muavic_dataset/train.wrd is sequence label. skipped
[2025-03-13 15:11:05,870][src.vsp_llm_dataset][INFO] - image transform: Compose(
    Normalize(mean=0.0, std=255.0)
    RandomCrop(size=(88, 88))
    <src.utils_vsp_llm.HorizontalFlip object at 0x7fa0a023d570>
    Normalize(mean=0.421, std=0.165)
)
[2025-03-13 15:11:05,870][src.vsp_llm_dataset][INFO] - pad_audio=True, random_crop=False, normalize=True, max_sample_size=500, seqs2seq data=True,
[2025-03-13 15:11:05,870][src.vsp_llm_dataset][INFO] - Noise wav: None->0 wav, Prob: 0.0, SNR: 0, Number of mixture: 1
[2025-03-13 15:11:06,109][fairseq.trainer][INFO] - begin training epoch 1
[2025-03-13 15:11:06,110][fairseq_cli.train][INFO] - Start iterating over samples
[2025-03-13 15:11:06,110][root][INFO] - Start iterating over samples
/home/jupyter-samantha_caasi@dls-bf571/models/VSP-LLM/fairseq/fairseq/tasks/fairseq_task.py:501: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
[2025-03-13 16:13:12,466][train_inner][INFO] - {"epoch": 1, "update": 0.052, "loss": "6.564", "total": "122.985", "n_correct": "28.685", "ppl": "94.65", "accuracy": "23.324", "wps": "6.6", "ups": "0.05", "wpb": "123", "bsz": "8", "num_updates": "200", "lr": "1.49e-05", "gnorm": "11.844", "loss_scale": "128", "train_wall": "3723", "gb_free": "21", "wall": "3727"}
[2025-03-13 16:13:12,467][root][INFO] - Step 200 | Loss: 6.5640 | Accuracy: 23.32 | LR: 0.000015
[2025-03-13 17:14:45,624][train_inner][INFO] - {"epoch": 1, "update": 0.104, "loss": "4.371", "total": "121.88", "n_correct": "41.595", "ppl": "20.69", "accuracy": "34.128", "wps": "6.6", "ups": "0.05", "wpb": "121.9", "bsz": "8", "num_updates": "400", "lr": "2.48e-05", "gnorm": "5.912", "loss_scale": "128", "train_wall": "3690", "gb_free": "21", "wall": "7420"}
[2025-03-13 17:14:45,625][root][INFO] - Step 400 | Loss: 4.3710 | Accuracy: 34.13 | LR: 0.000025
[2025-03-13 18:16:54,503][train_inner][INFO] - {"epoch": 1, "update": 0.156, "loss": "4.185", "total": "122.445", "n_correct": "43.515", "ppl": "18.18", "accuracy": "35.538", "wps": "6.6", "ups": "0.05", "wpb": "122.4", "bsz": "8", "num_updates": "600", "lr": "3.47e-05", "gnorm": "4.042", "loss_scale": "128", "train_wall": "3726", "gb_free": "21", "wall": "11149"}
[2025-03-13 18:16:54,503][root][INFO] - Step 600 | Loss: 4.1850 | Accuracy: 35.54 | LR: 0.000035
[2025-03-13 19:18:54,968][train_inner][INFO] - {"epoch": 1, "update": 0.208, "loss": "4.124", "total": "120.195", "n_correct": "43.615", "ppl": "17.43", "accuracy": "36.287", "wps": "6.5", "ups": "0.05", "wpb": "120.2", "bsz": "8", "num_updates": "800", "lr": "4.46e-05", "gnorm": "3.145", "loss_scale": "128", "train_wall": "3717", "gb_free": "21", "wall": "14869"}
[2025-03-13 19:18:54,968][root][INFO] - Step 800 | Loss: 4.1240 | Accuracy: 36.29 | LR: 0.000045
[2025-03-13 20:21:11,519][train_inner][INFO] - {"epoch": 1, "update": 0.26, "loss": "4.054", "total": "124.19", "n_correct": "45.29", "ppl": "16.61", "accuracy": "36.468", "wps": "6.6", "ups": "0.05", "wpb": "124.2", "bsz": "8", "num_updates": "1000", "lr": "5.45e-05", "gnorm": "2.724", "loss_scale": "128", "train_wall": "3734", "gb_free": "21", "wall": "18606"}
[2025-03-13 20:21:11,520][root][INFO] - Step 1000 | Loss: 4.0540 | Accuracy: 36.47 | LR: 0.000054
[2025-03-13 21:22:53,257][train_inner][INFO] - {"epoch": 1, "update": 0.312, "loss": "4.032", "total": "122.875", "n_correct": "45.255", "ppl": "16.36", "accuracy": "36.83", "wps": "6.6", "ups": "0.05", "wpb": "122.9", "bsz": "8", "num_updates": "1200", "lr": "6.44e-05", "gnorm": "2.318", "loss_scale": "128", "train_wall": "3699", "gb_free": "21", "wall": "22308"}
[2025-03-13 21:22:53,258][root][INFO] - Step 1200 | Loss: 4.0320 | Accuracy: 36.83 | LR: 0.000064
[2025-03-13 22:25:04,928][train_inner][INFO] - {"epoch": 1, "update": 0.364, "loss": "3.998", "total": "124.12", "n_correct": "46.585", "ppl": "15.98", "accuracy": "37.532", "wps": "6.7", "ups": "0.05", "wpb": "124.1", "bsz": "8", "num_updates": "1400", "lr": "7.43e-05", "gnorm": "2.179", "loss_scale": "128", "train_wall": "3729", "gb_free": "21", "wall": "26039"}
[2025-03-13 22:25:04,929][root][INFO] - Step 1400 | Loss: 3.9980 | Accuracy: 37.53 | LR: 0.000074
[2025-03-13 23:27:07,715][train_inner][INFO] - {"epoch": 1, "update": 0.416, "loss": "4.022", "total": "120.275", "n_correct": "44.325", "ppl": "16.24", "accuracy": "36.853", "wps": "6.5", "ups": "0.05", "wpb": "120.3", "bsz": "8", "num_updates": "1600", "lr": "8.42e-05", "gnorm": "1.822", "loss_scale": "128", "train_wall": "3720", "gb_free": "21", "wall": "29762"}
[2025-03-13 23:27:07,716][root][INFO] - Step 1600 | Loss: 4.0220 | Accuracy: 36.85 | LR: 0.000084
[2025-03-14 00:29:24,523][train_inner][INFO] - {"epoch": 1, "update": 0.468, "loss": "3.971", "total": "122.92", "n_correct": "46.155", "ppl": "15.69", "accuracy": "37.549", "wps": "6.6", "ups": "0.05", "wpb": "122.9", "bsz": "8", "num_updates": "1800", "lr": "9.41e-05", "gnorm": "1.578", "loss_scale": "128", "train_wall": "3734", "gb_free": "21", "wall": "33499"}
[2025-03-14 00:29:24,524][root][INFO] - Step 1800 | Loss: 3.9710 | Accuracy: 37.55 | LR: 0.000094
[2025-03-14 01:31:33,889][train_inner][INFO] - {"epoch": 1, "update": 0.52, "loss": "3.989", "total": "122.235", "n_correct": "46.135", "ppl": "15.88", "accuracy": "37.743", "wps": "6.6", "ups": "0.05", "wpb": "122.2", "bsz": "8", "num_updates": "2000", "lr": "0.000104", "gnorm": "1.503", "loss_scale": "128", "train_wall": "3726", "gb_free": "21", "wall": "37228"}
[2025-03-14 01:31:33,890][root][INFO] - Step 2000 | Loss: 3.9890 | Accuracy: 37.74 | LR: 0.000104
[2025-03-14 02:32:36,527][train_inner][INFO] - {"epoch": 1, "update": 0.572, "loss": "3.974", "total": "121.96", "n_correct": "46.275", "ppl": "15.71", "accuracy": "37.943", "wps": "6.7", "ups": "0.05", "wpb": "122", "bsz": "8", "num_updates": "2200", "lr": "0.0001139", "gnorm": "1.449", "loss_scale": "256", "train_wall": "3660", "gb_free": "21", "wall": "40891"}
[2025-03-14 02:32:36,527][root][INFO] - Step 2200 | Loss: 3.9740 | Accuracy: 37.94 | LR: 0.000114
[2025-03-14 03:22:08,372][train_inner][INFO] - {"epoch": 1, "update": 0.624, "loss": "3.945", "total": "119.8", "n_correct": "46.07", "ppl": "15.4", "accuracy": "38.456", "wps": "8.1", "ups": "0.07", "wpb": "119.8", "bsz": "8", "num_updates": "2400", "lr": "0.0001238", "gnorm": "1.535", "loss_scale": "256", "train_wall": "2969", "gb_free": "21", "wall": "43863"}
[2025-03-14 03:22:08,372][root][INFO] - Step 2400 | Loss: 3.9450 | Accuracy: 38.46 | LR: 0.000124
[2025-03-14 03:47:00,286][fairseq_cli.train][INFO] - begin validation on "test" subset
[2025-03-14 04:07:26,317][test][INFO] - {"epoch": 1, "test_loss": "nan", "test_total": "10.7964", "test_n_correct": "4.95988", "test_ppl": "nan", "test_accuracy": "45.94", "test_wps": "11.6", "test_wpb": "10.8", "test_bsz": "1", "test_num_updates": "2500"}
[2025-03-14 04:07:26,319][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1 @ 2500 updates
[2025-03-14 04:07:26,320][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_1_2500.pt
[2025-03-14 04:07:37,663][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_1_2500.pt
[2025-03-14 04:07:47,718][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_1_2500.pt (epoch 1 @ 2500 updates, score 45.94) (writing took 21.39907088689506 seconds)
[2025-03-14 04:32:39,183][train_inner][INFO] - {"epoch": 1, "update": 0.676, "loss": "3.88", "total": "122.105", "n_correct": "47.825", "ppl": "14.73", "accuracy": "39.167", "wps": "5.8", "ups": "0.05", "wpb": "122.1", "bsz": "8", "num_updates": "2600", "lr": "0.0001337", "gnorm": "1.866", "loss_scale": "256", "train_wall": "2981", "gb_free": "21", "wall": "48094"}
[2025-03-14 04:32:39,183][root][INFO] - Step 2600 | Loss: 3.8800 | Accuracy: 39.17 | LR: 0.000134
[2025-03-14 05:22:08,511][train_inner][INFO] - {"epoch": 1, "update": 0.728, "loss": "3.656", "total": "120.535", "n_correct": "50.81", "ppl": "12.6", "accuracy": "42.154", "wps": "8.1", "ups": "0.07", "wpb": "120.5", "bsz": "8", "num_updates": "2800", "lr": "0.0001436", "gnorm": "2.2", "loss_scale": "256", "train_wall": "2967", "gb_free": "21", "wall": "51063"}
[2025-03-14 05:22:08,512][root][INFO] - Step 2800 | Loss: 3.6560 | Accuracy: 42.15 | LR: 0.000144
[2025-03-14 06:11:46,580][train_inner][INFO] - {"epoch": 1, "update": 0.78, "loss": "3.566", "total": "122.43", "n_correct": "53.71", "ppl": "11.84", "accuracy": "43.87", "wps": "8.2", "ups": "0.07", "wpb": "122.4", "bsz": "8", "num_updates": "3000", "lr": "0.0001535", "gnorm": "2.043", "loss_scale": "256", "train_wall": "2975", "gb_free": "21", "wall": "54041"}
[2025-03-14 06:11:46,581][root][INFO] - Step 3000 | Loss: 3.5660 | Accuracy: 43.87 | LR: 0.000154
[2025-03-14 07:01:14,106][train_inner][INFO] - {"epoch": 1, "update": 0.832, "loss": "3.462", "total": "121.09", "n_correct": "53.985", "ppl": "11.02", "accuracy": "44.583", "wps": "8.2", "ups": "0.07", "wpb": "121.1", "bsz": "8", "num_updates": "3200", "lr": "0.0001634", "gnorm": "1.954", "loss_scale": "256", "train_wall": "2965", "gb_free": "21", "wall": "57009"}
[2025-03-14 07:01:14,106][root][INFO] - Step 3200 | Loss: 3.4620 | Accuracy: 44.58 | LR: 0.000163
[2025-03-14 07:56:28,569][train_inner][INFO] - {"epoch": 1, "update": 0.884, "loss": "3.442", "total": "121.15", "n_correct": "54.73", "ppl": "10.87", "accuracy": "45.175", "wps": "7.3", "ups": "0.06", "wpb": "121.2", "bsz": "8", "num_updates": "3400", "lr": "0.0001733", "gnorm": "1.816", "loss_scale": "256", "train_wall": "3312", "gb_free": "21", "wall": "60323"}
[2025-03-14 07:56:28,569][root][INFO] - Step 3400 | Loss: 3.4420 | Accuracy: 45.17 | LR: 0.000173
[2025-03-14 09:02:17,628][train_inner][INFO] - {"epoch": 1, "update": 0.936, "loss": "3.482", "total": "122.595", "n_correct": "54.97", "ppl": "11.17", "accuracy": "44.839", "wps": "6.2", "ups": "0.05", "wpb": "122.6", "bsz": "8", "num_updates": "3600", "lr": "0.0001832", "gnorm": "1.702", "loss_scale": "256", "train_wall": "3946", "gb_free": "21", "wall": "64272"}
[2025-03-14 09:02:17,629][root][INFO] - Step 3600 | Loss: 3.4820 | Accuracy: 44.84 | LR: 0.000183
[2025-03-14 09:55:07,164][train_inner][INFO] - {"epoch": 1, "update": 0.988, "loss": "3.391", "total": "120.96", "n_correct": "55.445", "ppl": "10.49", "accuracy": "45.837", "wps": "7.6", "ups": "0.06", "wpb": "121", "bsz": "8", "num_updates": "3800", "lr": "0.0001931", "gnorm": "1.645", "loss_scale": "256", "train_wall": "3167", "gb_free": "21", "wall": "67442"}
[2025-03-14 09:55:07,164][root][INFO] - Step 3800 | Loss: 3.3910 | Accuracy: 45.84 | LR: 0.000193
[2025-03-14 10:06:52,858][fairseq_cli.train][INFO] - begin validation on "test" subset
[2025-03-14 10:28:31,226][test][INFO] - {"epoch": 1, "test_loss": "nan", "test_total": "10.7964", "test_n_correct": "6.81832", "test_ppl": "nan", "test_accuracy": "63.154", "test_wps": "11", "test_wpb": "10.8", "test_bsz": "1", "test_num_updates": "3848", "test_best_accuracy": "63.154"}
[2025-03-14 10:28:31,229][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1 @ 3848 updates
[2025-03-14 10:28:31,229][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_best.pt
[2025-03-14 10:28:47,540][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_best.pt
[2025-03-14 10:28:54,497][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 1 @ 3848 updates, score 63.154) (writing took 23.268216583877802 seconds)
[2025-03-14 10:28:54,498][fairseq_cli.train][INFO] - end of epoch 1 (average epoch stats below)
[2025-03-14 10:28:54,510][train][INFO] - {"epoch": 1, "train_loss": "3.999", "train_total": "121.887", "train_n_correct": "47.1668", "train_ppl": "15.98", "train_accuracy": "38.697", "train_wps": "6.8", "train_ups": "0.06", "train_wpb": "121.9", "train_bsz": "8", "train_num_updates": "3848", "train_lr": "0.000195476", "train_gnorm": "2.789", "train_loss_scale": "256", "train_train_wall": "66844", "train_gb_free": "21", "train_wall": "69469"}
[2025-03-14 10:28:54,510][root][INFO] - Training complete. Final stats: OrderedDict([('loss', 3.999), ('total', 121.88747401247402), ('n_correct', 47.16683991683992), ('ppl', 15.98), ('accuracy', 38.697), ('wps', 6.8), ('ups', 0.06), ('wpb', 121.9), ('bsz', 8.0), ('num_updates', 3848), ('lr', 0.000195476), ('gnorm', 2.789), ('loss_scale', 256.0), ('train_wall', 66844.0), ('gb_free', 21.0), ('wall', 69469.0)])
[2025-03-14 10:28:54,653][fairseq.trainer][INFO] - begin training epoch 2
[2025-03-14 10:28:54,653][fairseq_cli.train][INFO] - Start iterating over samples
[2025-03-14 10:28:54,653][root][INFO] - Start iterating over samples
[2025-03-14 11:18:51,369][train_inner][INFO] - {"epoch": 2, "update": 1.04, "loss": "3.408", "total": "122.06", "n_correct": "55.59", "ppl": "10.62", "accuracy": "45.543", "wps": "4.9", "ups": "0.04", "wpb": "122.1", "bsz": "8", "num_updates": "4000", "lr": "0.000203", "gnorm": "1.615", "loss_scale": "256", "train_wall": "3699", "gb_free": "21", "wall": "72466"}
[2025-03-14 11:18:51,370][root][INFO] - Step 4000 | Loss: 3.4080 | Accuracy: 45.54 | LR: 0.000203
^[[15~^[[15~[2025-03-14 12:24:17,928][train_inner][INFO] - {"epoch": 2, "update": 1.091, "loss": "3.366", "total": "121.56", "n_correct": "56.05", "ppl": "10.31", "accuracy": "46.109", "wps": "6.2", "ups": "0.05", "wpb": "121.6", "bsz": "8", "num_updates": "4200", "lr": "0.0002129", "gnorm": "1.572", "loss_scale": "512", "train_wall": "3924", "gb_free": "21", "wall": "76392"}
[2025-03-14 12:24:17,929][root][INFO] - Step 4200 | Loss: 3.3660 | Accuracy: 46.11 | LR: 0.000213
[2025-03-14 13:16:17,019][train_inner][INFO] - {"epoch": 2, "update": 1.143, "loss": "3.322", "total": "121.545", "n_correct": "56.73", "ppl": "10", "accuracy": "46.674", "wps": "7.8", "ups": "0.06", "wpb": "121.5", "bsz": "8", "num_updates": "4400", "lr": "0.0002228", "gnorm": "1.649", "loss_scale": "512", "train_wall": "3117", "gb_free": "21", "wall": "79511"}
[2025-03-14 13:16:17,019][root][INFO] - Step 4400 | Loss: 3.3220 | Accuracy: 46.67 | LR: 0.000223
[2025-03-14 14:05:49,870][train_inner][INFO] - {"epoch": 2, "update": 1.195, "loss": "3.36", "total": "122.335", "n_correct": "56.185", "ppl": "10.26", "accuracy": "45.927", "wps": "8.2", "ups": "0.07", "wpb": "122.3", "bsz": "8", "num_updates": "4600", "lr": "0.0002327", "gnorm": "1.566", "loss_scale": "512", "train_wall": "2970", "gb_free": "21", "wall": "82484"}
[2025-03-14 14:05:49,870][root][INFO] - Step 4600 | Loss: 3.3600 | Accuracy: 45.93 | LR: 0.000233
[2025-03-14 14:55:22,325][train_inner][INFO] - {"epoch": 2, "update": 1.247, "loss": "3.299", "total": "121.405", "n_correct": "56.825", "ppl": "9.84", "accuracy": "46.806", "wps": "8.2", "ups": "0.07", "wpb": "121.4", "bsz": "8", "num_updates": "4800", "lr": "0.0002426", "gnorm": "1.5", "loss_scale": "512", "train_wall": "2970", "gb_free": "21", "wall": "85457"}
[2025-03-14 14:55:22,325][root][INFO] - Step 4800 | Loss: 3.2990 | Accuracy: 46.81 | LR: 0.000243
[2025-03-14 15:45:17,859][train_inner][INFO] - {"epoch": 2, "update": 1.299, "loss": "3.312", "total": "124.63", "n_correct": "58.4", "ppl": "9.93", "accuracy": "46.859", "wps": "8.3", "ups": "0.07", "wpb": "124.6", "bsz": "8", "num_updates": "5000", "lr": "0.0002525", "gnorm": "1.507", "loss_scale": "512", "train_wall": "2993", "gb_free": "21", "wall": "88452"}
[2025-03-14 15:45:17,860][root][INFO] - Step 5000 | Loss: 3.3120 | Accuracy: 46.86 | LR: 0.000253
[2025-03-14 15:45:17,861][fairseq_cli.train][INFO] - begin validation on "test" subset
[2025-03-14 16:05:45,226][test][INFO] - {"epoch": 2, "test_loss": "nan", "test_total": "10.7964", "test_n_correct": "6.92354", "test_ppl": "nan", "test_accuracy": "64.128", "test_wps": "11.6", "test_wpb": "10.8", "test_bsz": "1", "test_num_updates": "5000", "test_best_accuracy": "64.128"}
[2025-03-14 16:05:45,228][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 2 @ 5000 updates
[2025-03-14 16:05:45,230][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_2_5000.pt
[2025-03-14 16:05:58,010][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_2_5000.pt
[2025-03-14 16:06:10,656][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_2_5000.pt (epoch 2 @ 5000 updates, score 64.128) (writing took 25.42707579396665 seconds)
[2025-03-14 16:55:46,177][train_inner][INFO] - {"epoch": 2, "update": 1.351, "loss": "3.354", "total": "122.17", "n_correct": "56.605", "ppl": "10.23", "accuracy": "46.333", "wps": "5.8", "ups": "0.05", "wpb": "122.2", "bsz": "8", "num_updates": "5200", "lr": "0.0002624", "gnorm": "1.474", "loss_scale": "512", "train_wall": "2973", "gb_free": "21", "wall": "92681"}
[2025-03-14 16:55:46,177][root][INFO] - Step 5200 | Loss: 3.3540 | Accuracy: 46.33 | LR: 0.000262
[2025-03-14 17:45:29,462][train_inner][INFO] - {"epoch": 2, "update": 1.403, "loss": "3.356", "total": "123.665", "n_correct": "56.905", "ppl": "10.24", "accuracy": "46.015", "wps": "8.3", "ups": "0.07", "wpb": "123.7", "bsz": "8", "num_updates": "5400", "lr": "0.0002723", "gnorm": "1.491", "loss_scale": "512", "train_wall": "2981", "gb_free": "21", "wall": "95664"}
[2025-03-14 17:45:29,463][root][INFO] - Step 5400 | Loss: 3.3560 | Accuracy: 46.02 | LR: 0.000272
[2025-03-14 18:43:27,258][train_inner][INFO] - {"epoch": 2, "update": 1.455, "loss": "3.298", "total": "119.845", "n_correct": "56.005", "ppl": "9.83", "accuracy": "46.731", "wps": "6.9", "ups": "0.06", "wpb": "119.8", "bsz": "8", "num_updates": "5600", "lr": "0.0002822", "gnorm": "1.487", "loss_scale": "512", "train_wall": "3475", "gb_free": "21", "wall": "99142"}
[2025-03-14 18:43:27,258][root][INFO] - Step 5600 | Loss: 3.2980 | Accuracy: 46.73 | LR: 0.000282
[2025-03-14 19:54:42,562][train_inner][INFO] - {"epoch": 2, "update": 1.507, "loss": "3.354", "total": "121.335", "n_correct": "56.02", "ppl": "10.23", "accuracy": "46.17", "wps": "5.7", "ups": "0.05", "wpb": "121.3", "bsz": "8", "num_updates": "5800", "lr": "0.0002921", "gnorm": "1.478", "loss_scale": "512", "train_wall": "4273", "gb_free": "21", "wall": "103417"}
[2025-03-14 19:54:42,563][root][INFO] - Step 5800 | Loss: 3.3540 | Accuracy: 46.17 | LR: 0.000292
[2025-03-14 21:07:30,499][train_inner][INFO] - {"epoch": 2, "update": 1.559, "loss": "3.337", "total": "124.12", "n_correct": "56.865", "ppl": "10.11", "accuracy": "45.815", "wps": "5.7", "ups": "0.05", "wpb": "124.1", "bsz": "8", "num_updates": "6000", "lr": "0.000302", "gnorm": "1.47", "loss_scale": "512", "train_wall": "4365", "gb_free": "21", "wall": "107785"}
[2025-03-14 21:07:30,499][root][INFO] - Step 6000 | Loss: 3.3370 | Accuracy: 45.81 | LR: 0.000302
[2025-03-14 22:20:48,038][train_inner][INFO] - {"epoch": 2, "update": 1.611, "loss": "3.352", "total": "123.36", "n_correct": "56.955", "ppl": "10.21", "accuracy": "46.17", "wps": "5.6", "ups": "0.05", "wpb": "123.4", "bsz": "8", "num_updates": "6200", "lr": "0.0003119", "gnorm": "1.488", "loss_scale": "1024", "train_wall": "4395", "gb_free": "21", "wall": "112182"}
[2025-03-14 22:20:48,039][root][INFO] - Step 6200 | Loss: 3.3520 | Accuracy: 46.17 | LR: 0.000312
[2025-03-14 23:11:17,408][train_inner][INFO] - {"epoch": 2, "update": 1.663, "loss": "3.305", "total": "122.42", "n_correct": "57.275", "ppl": "9.89", "accuracy": "46.786", "wps": "8.1", "ups": "0.07", "wpb": "122.4", "bsz": "8", "num_updates": "6400", "lr": "0.0003218", "gnorm": "1.466", "loss_scale": "1024", "train_wall": "3027", "gb_free": "21", "wall": "115212"}
[2025-03-14 23:11:17,408][root][INFO] - Step 6400 | Loss: 3.3050 | Accuracy: 46.79 | LR: 0.000322
[2025-03-15 00:00:16,248][train_inner][INFO] - {"epoch": 2, "update": 1.715, "loss": "3.263", "total": "122.02", "n_correct": "57.285", "ppl": "9.6", "accuracy": "46.947", "wps": "8.3", "ups": "0.07", "wpb": "122", "bsz": "8", "num_updates": "6600", "lr": "0.0003317", "gnorm": "1.477", "loss_scale": "1024", "train_wall": "2937", "gb_free": "21", "wall": "118151"}
[2025-03-15 00:00:16,249][root][INFO] - Step 6600 | Loss: 3.2630 | Accuracy: 46.95 | LR: 0.000332
[2025-03-15 00:49:27,256][train_inner][INFO] - {"epoch": 2, "update": 1.767, "loss": "3.302", "total": "122.595", "n_correct": "57.495", "ppl": "9.86", "accuracy": "46.898", "wps": "8.3", "ups": "0.07", "wpb": "122.6", "bsz": "8", "num_updates": "6800", "lr": "0.0003416", "gnorm": "1.484", "loss_scale": "1024", "train_wall": "2948", "gb_free": "21", "wall": "121102"}
[2025-03-15 00:49:27,257][root][INFO] - Step 6800 | Loss: 3.3020 | Accuracy: 46.90 | LR: 0.000342
[2025-03-15 01:38:50,198][train_inner][INFO] - {"epoch": 2, "update": 1.819, "loss": "3.33", "total": "124.61", "n_correct": "58.595", "ppl": "10.05", "accuracy": "47.023", "wps": "8.4", "ups": "0.07", "wpb": "124.6", "bsz": "8", "num_updates": "7000", "lr": "0.0003515", "gnorm": "1.52", "loss_scale": "1024", "train_wall": "2960", "gb_free": "21", "wall": "124065"}
[2025-03-15 01:38:50,198][root][INFO] - Step 7000 | Loss: 3.3300 | Accuracy: 47.02 | LR: 0.000352
[2025-03-15 02:27:42,635][train_inner][INFO] - {"epoch": 2, "update": 1.871, "loss": "3.36", "total": "119.495", "n_correct": "55.015", "ppl": "10.27", "accuracy": "46.04", "wps": "8.1", "ups": "0.07", "wpb": "119.5", "bsz": "8", "num_updates": "7200", "lr": "0.0003614", "gnorm": "1.541", "loss_scale": "1024", "train_wall": "2930", "gb_free": "21", "wall": "126997"}
[2025-03-15 02:27:42,635][root][INFO] - Step 7200 | Loss: 3.3600 | Accuracy: 46.04 | LR: 0.000361
[2025-03-15 03:16:42,900][train_inner][INFO] - {"epoch": 2, "update": 1.923, "loss": "3.324", "total": "119.935", "n_correct": "56.14", "ppl": "10.02", "accuracy": "46.809", "wps": "8.2", "ups": "0.07", "wpb": "119.9", "bsz": "8", "num_updates": "7400", "lr": "0.0003713", "gnorm": "1.583", "loss_scale": "1024", "train_wall": "2938", "gb_free": "21", "wall": "129937"}
[2025-03-15 03:16:42,900][root][INFO] - Step 7400 | Loss: 3.3240 | Accuracy: 46.81 | LR: 0.000371
[2025-03-15 03:41:01,447][fairseq_cli.train][INFO] - begin validation on "test" subset
[2025-03-15 04:01:12,182][test][INFO] - {"epoch": 2, "test_loss": "nan", "test_total": "10.7964", "test_n_correct": "6.9349", "test_ppl": "nan", "test_accuracy": "64.234", "test_wps": "11.8", "test_wpb": "10.8", "test_bsz": "1", "test_num_updates": "7500", "test_best_accuracy": "64.234"}
[2025-03-15 04:01:12,185][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 2 @ 7500 updates
[2025-03-15 04:01:12,185][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_2_7500.pt
[2025-03-15 04:01:22,766][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_2_7500.pt
[2025-03-15 04:01:33,242][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_2_7500.pt (epoch 2 @ 7500 updates, score 64.234) (writing took 21.05704246275127 seconds)
[2025-03-15 04:25:57,159][train_inner][INFO] - {"epoch": 2, "update": 1.975, "loss": "3.361", "total": "117.485", "n_correct": "53.58", "ppl": "10.28", "accuracy": "45.606", "wps": "5.7", "ups": "0.05", "wpb": "117.5", "bsz": "8", "num_updates": "7600", "lr": "0.0003812", "gnorm": "1.597", "loss_scale": "1024", "train_wall": "2920", "gb_free": "21", "wall": "134092"}
[2025-03-15 04:25:57,160][root][INFO] - Step 7600 | Loss: 3.3610 | Accuracy: 45.61 | LR: 0.000381
[2025-03-15 04:49:16,650][fairseq_cli.train][INFO] - begin validation on "test" subset
[2025-03-15 05:09:31,175][test][INFO] - {"epoch": 2, "test_loss": "nan", "test_total": "10.7964", "test_n_correct": "6.32097", "test_ppl": "nan", "test_accuracy": "58.547", "test_wps": "11.7", "test_wpb": "10.8", "test_bsz": "1", "test_num_updates": "7696", "test_best_accuracy": "64.234"}
[2025-03-15 05:09:31,176][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 2 @ 7696 updates
[2025-03-15 05:09:31,177][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2025-03-15 05:09:45,189][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2025-03-15 05:09:45,563][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 2 @ 7696 updates, score 58.547) (writing took 14.386850502341986 seconds)
[2025-03-15 05:09:45,565][fairseq_cli.train][INFO] - end of epoch 2 (average epoch stats below)
[2025-03-15 05:09:45,575][train][INFO] - {"epoch": 2, "train_loss": "3.334", "train_total": "121.887", "train_n_correct": "56.5743", "train_ppl": "10.08", "train_accuracy": "46.415", "train_wps": "7", "train_ups": "0.06", "train_wpb": "121.9", "train_bsz": "8", "train_num_updates": "7696", "train_lr": "0.000385952", "train_gnorm": "1.527", "train_loss_scale": "1024", "train_train_wall": "63488", "train_gb_free": "21", "train_wall": "136720"}
[2025-03-15 05:09:45,575][root][INFO] - Training complete. Final stats: OrderedDict([('loss', 3.334), ('total', 121.88747401247402), ('n_correct', 56.57432432432432), ('ppl', 10.08), ('accuracy', 46.415), ('wps', 7.0), ('ups', 0.06), ('wpb', 121.9), ('bsz', 8.0), ('num_updates', 7696), ('lr', 0.000385952), ('gnorm', 1.527), ('loss_scale', 1024.0), ('train_wall', 63488.0), ('gb_free', 21.0), ('wall', 136720.0)])
[2025-03-15 05:09:45,691][fairseq.trainer][INFO] - begin training epoch 3
[2025-03-15 05:09:45,691][fairseq_cli.train][INFO] - Start iterating over samples
[2025-03-15 05:09:45,691][root][INFO] - Start iterating over samples
[2025-03-15 05:35:15,117][train_inner][INFO] - {"epoch": 3, "update": 2.027, "loss": "3.237", "total": "120.505", "n_correct": "57.155", "ppl": "9.43", "accuracy": "47.43", "wps": "5.8", "ups": "0.05", "wpb": "120.5", "bsz": "8", "num_updates": "7800", "lr": "0.0003911", "gnorm": "1.676", "loss_scale": "1024", "train_wall": "2926", "gb_free": "21", "wall": "138250"}
[2025-03-15 05:35:15,117][root][INFO] - Step 7800 | Loss: 3.2370 | Accuracy: 47.43 | LR: 0.000391
[2025-03-15 06:24:16,662][train_inner][INFO] - {"epoch": 3, "update": 2.079, "loss": "3.2", "total": "122.01", "n_correct": "58.42", "ppl": "9.19", "accuracy": "47.881", "wps": "8.3", "ups": "0.07", "wpb": "122", "bsz": "8", "num_updates": "8000", "lr": "0.000401", "gnorm": "1.738", "loss_scale": "1024", "train_wall": "2939", "gb_free": "21", "wall": "141191"}
[2025-03-15 06:24:16,663][root][INFO] - Step 8000 | Loss: 3.2000 | Accuracy: 47.88 | LR: 0.000401
[2025-03-15 07:13:35,505][train_inner][INFO] - {"epoch": 3, "update": 2.131, "loss": "3.163", "total": "123.865", "n_correct": "59.565", "ppl": "8.95", "accuracy": "48.089", "wps": "8.4", "ups": "0.07", "wpb": "123.9", "bsz": "8", "num_updates": "8200", "lr": "0.0004109", "gnorm": "1.795", "loss_scale": "2048", "train_wall": "2956", "gb_free": "21", "wall": "144150"}
[2025-03-15 07:13:35,506][root][INFO] - Step 8200 | Loss: 3.1630 | Accuracy: 48.09 | LR: 0.000411
[2025-03-15 08:07:28,307][train_inner][INFO] - {"epoch": 3, "update": 2.183, "loss": "3.176", "total": "121.91", "n_correct": "58.15", "ppl": "9.04", "accuracy": "47.699", "wps": "7.5", "ups": "0.06", "wpb": "121.9", "bsz": "8", "num_updates": "8400", "lr": "0.0004208", "gnorm": "1.82", "loss_scale": "2048", "train_wall": "3230", "gb_free": "21", "wall": "147383"}
[2025-03-15 08:07:28,307][root][INFO] - Step 8400 | Loss: 3.1760 | Accuracy: 47.70 | LR: 0.000421
[2025-03-15 09:15:03,300][train_inner][INFO] - {"epoch": 3, "update": 2.235, "loss": "3.23", "total": "120.175", "n_correct": "56.675", "ppl": "9.38", "accuracy": "47.16", "wps": "5.9", "ups": "0.05", "wpb": "120.2", "bsz": "8", "num_updates": "8600", "lr": "0.0004307", "gnorm": "1.903", "loss_scale": "2048", "train_wall": "4052", "gb_free": "21", "wall": "151438"}
[2025-03-15 09:15:03,300][root][INFO] - Step 8600 | Loss: 3.2300 | Accuracy: 47.16 | LR: 0.000431
[2025-03-15 10:23:08,652][train_inner][INFO] - {"epoch": 3, "update": 2.287, "loss": "3.205", "total": "123.635", "n_correct": "58.905", "ppl": "9.22", "accuracy": "47.644", "wps": "6.1", "ups": "0.05", "wpb": "123.6", "bsz": "8", "num_updates": "8800", "lr": "0.0004406", "gnorm": "1.9", "loss_scale": "2048", "train_wall": "4083", "gb_free": "21", "wall": "155523"}
[2025-03-15 10:23:08,653][root][INFO] - Step 8800 | Loss: 3.2050 | Accuracy: 47.64 | LR: 0.000441
[2025-03-15 11:30:59,590][train_inner][INFO] - {"epoch": 3, "update": 2.339, "loss": "3.211", "total": "121.84", "n_correct": "57.375", "ppl": "9.26", "accuracy": "47.09", "wps": "6", "ups": "0.05", "wpb": "121.8", "bsz": "8", "num_updates": "9000", "lr": "0.0004505", "gnorm": "1.961", "loss_scale": "2048", "train_wall": "4068", "gb_free": "21", "wall": "159594"}
[2025-03-15 11:30:59,590][root][INFO] - Step 9000 | Loss: 3.2110 | Accuracy: 47.09 | LR: 0.000450
[2025-03-15 12:39:17,789][train_inner][INFO] - {"epoch": 3, "update": 2.391, "loss": "3.207", "total": "123.98", "n_correct": "59.27", "ppl": "9.23", "accuracy": "47.806", "wps": "6.1", "ups": "0.05", "wpb": "124", "bsz": "8", "num_updates": "9200", "lr": "0.0004604", "gnorm": "1.932", "loss_scale": "2048", "train_wall": "4095", "gb_free": "21", "wall": "163692"}
[2025-03-15 12:39:17,790][root][INFO] - Step 9200 | Loss: 3.2070 | Accuracy: 47.81 | LR: 0.000460
[2025-03-15 13:47:48,417][train_inner][INFO] - {"epoch": 3, "update": 2.443, "loss": "3.245", "total": "124.67", "n_correct": "58.38", "ppl": "9.48", "accuracy": "46.828", "wps": "6.1", "ups": "0.05", "wpb": "124.7", "bsz": "8", "num_updates": "9400", "lr": "0.0004703", "gnorm": "1.984", "loss_scale": "2048", "train_wall": "4108", "gb_free": "21", "wall": "167803"}
[2025-03-15 13:47:48,417][root][INFO] - Step 9400 | Loss: 3.2450 | Accuracy: 46.83 | LR: 0.000470
[2025-03-15 14:55:52,847][train_inner][INFO] - {"epoch": 3, "update": 2.495, "loss": "3.235", "total": "119.225", "n_correct": "56.415", "ppl": "9.42", "accuracy": "47.318", "wps": "5.8", "ups": "0.05", "wpb": "119.2", "bsz": "8", "num_updates": "9600", "lr": "0.0004802", "gnorm": "2.075", "loss_scale": "2048", "train_wall": "4081", "gb_free": "21", "wall": "171887"}
[2025-03-15 14:55:52,847][root][INFO] - Step 9600 | Loss: 3.2350 | Accuracy: 47.32 | LR: 0.000480
[2025-03-15 16:03:54,234][train_inner][INFO] - {"epoch": 3, "update": 2.547, "loss": "3.229", "total": "119.905", "n_correct": "55.95", "ppl": "9.38", "accuracy": "46.662", "wps": "5.9", "ups": "0.05", "wpb": "119.9", "bsz": "8", "num_updates": "9800", "lr": "0.0004901", "gnorm": "2.087", "loss_scale": "2048", "train_wall": "4078", "gb_free": "21", "wall": "175969"}
[2025-03-15 16:03:54,234][root][INFO] - Step 9800 | Loss: 3.2290 | Accuracy: 46.66 | LR: 0.000490
[2025-03-15 17:12:14,654][train_inner][INFO] - {"epoch": 3, "update": 2.599, "loss": "3.279", "total": "122.475", "n_correct": "57.455", "ppl": "9.71", "accuracy": "46.912", "wps": "6", "ups": "0.05", "wpb": "122.5", "bsz": "8", "num_updates": "10000", "lr": "0.0005", "gnorm": "2.098", "loss_scale": "2048", "train_wall": "4097", "gb_free": "21", "wall": "180069"}
[2025-03-15 17:12:14,655][root][INFO] - Step 10000 | Loss: 3.2790 | Accuracy: 46.91 | LR: 0.000500
[2025-03-15 17:12:14,656][fairseq_cli.train][INFO] - begin validation on "test" subset
[2025-03-15 17:40:23,895][test][INFO] - {"epoch": 3, "test_loss": "nan", "test_total": "10.7964", "test_n_correct": "7.07419", "test_ppl": "nan", "test_accuracy": "65.524", "test_wps": "8.4", "test_wpb": "10.8", "test_bsz": "1", "test_num_updates": "10000", "test_best_accuracy": "65.524"}
[2025-03-15 17:40:23,897][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 3 @ 10000 updates
[2025-03-15 17:40:23,898][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_3_10000.pt
[2025-03-15 17:40:38,360][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_3_10000.pt
[2025-03-15 17:40:52,524][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_3_10000.pt (epoch 3 @ 10000 updates, score 65.524) (writing took 28.62634284980595 seconds)
[2025-03-15 18:49:19,512][train_inner][INFO] - {"epoch": 3, "update": 2.651, "loss": "3.24", "total": "122.64", "n_correct": "57.435", "ppl": "9.45", "accuracy": "46.832", "wps": "4.2", "ups": "0.03", "wpb": "122.6", "bsz": "8", "num_updates": "10200", "lr": "0.000485243", "gnorm": "2.137", "loss_scale": "2048", "train_wall": "4104", "gb_free": "21", "wall": "185894"}
[2025-03-15 18:49:19,513][root][INFO] - Step 10200 | Loss: 3.2400 | Accuracy: 46.83 | LR: 0.000485
[2025-03-15 19:57:25,145][train_inner][INFO] - {"epoch": 3, "update": 2.703, "loss": "3.294", "total": "120.13", "n_correct": "55.685", "ppl": "9.81", "accuracy": "46.354", "wps": "5.9", "ups": "0.05", "wpb": "120.1", "bsz": "8", "num_updates": "10400", "lr": "0.000470922", "gnorm": "2.184", "loss_scale": "4096", "train_wall": "4083", "gb_free": "21", "wall": "189980"}
[2025-03-15 19:57:25,146][root][INFO] - Step 10400 | Loss: 3.2940 | Accuracy: 46.35 | LR: 0.000471
[2025-03-15 20:05:12,903][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0
[2025-03-15 21:05:18,262][train_inner][INFO] - {"epoch": 3, "update": 2.755, "loss": "3.241", "total": "119.28", "n_correct": "55.475", "ppl": "9.46", "accuracy": "46.508", "wps": "5.9", "ups": "0.05", "wpb": "119.3", "bsz": "8", "num_updates": "10600", "lr": "0.000457024", "gnorm": "2.102", "loss_scale": "2048", "train_wall": "4070", "gb_free": "21", "wall": "194053"}
[2025-03-15 21:05:18,262][root][INFO] - Step 10600 | Loss: 3.2410 | Accuracy: 46.51 | LR: 0.000457
[2025-03-15 22:12:47,787][train_inner][INFO] - {"epoch": 3, "update": 2.807, "loss": "3.24", "total": "121.48", "n_correct": "56.92", "ppl": "9.45", "accuracy": "46.855", "wps": "6", "ups": "0.05", "wpb": "121.5", "bsz": "8", "num_updates": "10800", "lr": "0.000443536", "gnorm": "2.206", "loss_scale": "2048", "train_wall": "4047", "gb_free": "21", "wall": "198102"}
[2025-03-15 22:12:47,788][root][INFO] - Step 10800 | Loss: 3.2400 | Accuracy: 46.85 | LR: 0.000444
[2025-03-15 23:20:05,513][train_inner][INFO] - {"epoch": 3, "update": 2.859, "loss": "3.243", "total": "121.155", "n_correct": "57.645", "ppl": "9.47", "accuracy": "47.58", "wps": "6", "ups": "0.05", "wpb": "121.2", "bsz": "8", "num_updates": "11000", "lr": "0.000430446", "gnorm": "2.198", "loss_scale": "2048", "train_wall": "4035", "gb_free": "21", "wall": "202140"}
[2025-03-15 23:20:05,513][root][INFO] - Step 11000 | Loss: 3.2430 | Accuracy: 47.58 | LR: 0.000430
[2025-03-16 00:27:42,417][train_inner][INFO] - {"epoch": 3, "update": 2.911, "loss": "3.309", "total": "124.34", "n_correct": "57.845", "ppl": "9.91", "accuracy": "46.522", "wps": "6.1", "ups": "0.05", "wpb": "124.3", "bsz": "8", "num_updates": "11200", "lr": "0.000417742", "gnorm": "2.2", "loss_scale": "2048", "train_wall": "4055", "gb_free": "21", "wall": "206197"}
[2025-03-16 00:27:42,418][root][INFO] - Step 11200 | Loss: 3.3090 | Accuracy: 46.52 | LR: 0.000418
[2025-03-16 01:34:55,384][train_inner][INFO] - {"epoch": 3, "update": 2.963, "loss": "3.192", "total": "120.945", "n_correct": "57.3", "ppl": "9.14", "accuracy": "47.377", "wps": "6", "ups": "0.05", "wpb": "120.9", "bsz": "8", "num_updates": "11400", "lr": "0.000405413", "gnorm": "2.128", "loss_scale": "2048", "train_wall": "4031", "gb_free": "21", "wall": "210230"}
[2025-03-16 01:34:55,384][root][INFO] - Step 11400 | Loss: 3.1920 | Accuracy: 47.38 | LR: 0.000405
[2025-03-16 02:23:03,316][fairseq_cli.train][INFO] - begin validation on "test" subset
[2025-03-16 02:50:45,193][test][INFO] - {"epoch": 3, "test_loss": "nan", "test_total": "10.7964", "test_n_correct": "7.05829", "test_ppl": "nan", "test_accuracy": "65.377", "test_wps": "8.6", "test_wpb": "10.8", "test_bsz": "1", "test_num_updates": "11543", "test_best_accuracy": "65.524"}
[2025-03-16 02:50:45,194][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 3 @ 11543 updates
[2025-03-16 02:50:45,195][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_last.pt
[2025-03-16 02:51:01,362][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_last.pt
[2025-03-16 02:51:01,774][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 3 @ 11543 updates, score 65.377) (writing took 16.57922837883234 seconds)
[2025-03-16 02:51:01,775][fairseq_cli.train][INFO] - end of epoch 3 (average epoch stats below)
[2025-03-16 02:51:01,796][train][INFO] - {"epoch": 3, "train_loss": "3.23", "train_total": "121.891", "train_n_correct": "57.5269", "train_ppl": "9.38", "train_accuracy": "47.195", "train_wps": "6", "train_ups": "0.05", "train_wpb": "121.9", "train_bsz": "8", "train_num_updates": "11543", "train_lr": "0.000396822", "train_gnorm": "2.025", "train_loss_scale": "2048", "train_train_wall": "74628", "train_gb_free": "21", "train_wall": "214796"}
[2025-03-16 02:51:01,798][root][INFO] - Training complete. Final stats: OrderedDict([('loss', 3.23), ('total', 121.89082401871589), ('n_correct', 57.52690408110216), ('ppl', 9.38), ('accuracy', 47.195), ('wps', 6.0), ('ups', 0.05), ('wpb', 121.9), ('bsz', 8.0), ('num_updates', 11543), ('lr', 0.00039682181591246503), ('gnorm', 2.025), ('loss_scale', 2048.0), ('train_wall', 74628.0), ('gb_free', 21.0), ('wall', 214796.0)])
[2025-03-16 02:51:01,963][fairseq.trainer][INFO] - begin training epoch 4
[2025-03-16 02:51:01,963][fairseq_cli.train][INFO] - Start iterating over samples
[2025-03-16 02:51:01,963][root][INFO] - Start iterating over samples
[2025-03-16 03:10:21,867][train_inner][INFO] - {"epoch": 4, "update": 3.015, "loss": "3.188", "total": "123.075", "n_correct": "58.975", "ppl": "9.11", "accuracy": "47.918", "wps": "4.3", "ups": "0.03", "wpb": "123.1", "bsz": "8", "num_updates": "11600", "lr": "0.000393448", "gnorm": "2.246", "loss_scale": "2048", "train_wall": "4045", "gb_free": "21", "wall": "215956"}
[2025-03-16 03:10:21,868][root][INFO] - Step 11600 | Loss: 3.1880 | Accuracy: 47.92 | LR: 0.000393
[2025-03-16 04:17:40,540][train_inner][INFO] - {"epoch": 4, "update": 3.067, "loss": "2.99", "total": "120.91", "n_correct": "59.93", "ppl": "7.94", "accuracy": "49.566", "wps": "6", "ups": "0.05", "wpb": "120.9", "bsz": "8", "num_updates": "11800", "lr": "0.000381836", "gnorm": "2.312", "loss_scale": "2048", "train_wall": "4036", "gb_free": "21", "wall": "219995"}
[2025-03-16 04:17:40,541][root][INFO] - Step 11800 | Loss: 2.9900 | Accuracy: 49.57 | LR: 0.000382
[2025-03-16 05:24:24,614][train_inner][INFO] - {"epoch": 4, "update": 3.119, "loss": "2.97", "total": "122.045", "n_correct": "61.155", "ppl": "7.84", "accuracy": "50.109", "wps": "6.1", "ups": "0.05", "wpb": "122", "bsz": "8", "num_updates": "12000", "lr": "0.000370567", "gnorm": "2.343", "loss_scale": "2048", "train_wall": "4002", "gb_free": "21", "wall": "223999"}
[2025-03-16 05:24:24,615][root][INFO] - Step 12000 | Loss: 2.9700 | Accuracy: 50.11 | LR: 0.000371
[2025-03-16 06:31:18,034][train_inner][INFO] - {"epoch": 4, "update": 3.171, "loss": "2.966", "total": "123.61", "n_correct": "62.19", "ppl": "7.81", "accuracy": "50.311", "wps": "6.2", "ups": "0.05", "wpb": "123.6", "bsz": "8", "num_updates": "12200", "lr": "0.000359631", "gnorm": "2.418", "loss_scale": "2048", "train_wall": "4011", "gb_free": "21", "wall": "228012"}
[2025-03-16 06:31:18,034][root][INFO] - Step 12200 | Loss: 2.9660 | Accuracy: 50.31 | LR: 0.000360
[2025-03-16 07:37:54,700][train_inner][INFO] - {"epoch": 4, "update": 3.223, "loss": "2.917", "total": "121.64", "n_correct": "61.19", "ppl": "7.55", "accuracy": "50.304", "wps": "6.1", "ups": "0.05", "wpb": "121.6", "bsz": "8", "num_updates": "12400", "lr": "0.000349017", "gnorm": "2.378", "loss_scale": "2048", "train_wall": "3995", "gb_free": "21", "wall": "232009"}
[2025-03-16 07:37:54,700][root][INFO] - Step 12400 | Loss: 2.9170 | Accuracy: 50.30 | LR: 0.000349
[2025-03-16 08:11:05,621][fairseq_cli.train][INFO] - begin validation on "test" subset
[2025-03-16 08:38:38,530][test][INFO] - {"epoch": 4, "test_loss": "nan", "test_total": "10.7964", "test_n_correct": "7.02877", "test_ppl": "nan", "test_accuracy": "65.103", "test_wps": "8.6", "test_wpb": "10.8", "test_bsz": "1", "test_num_updates": "12500", "test_best_accuracy": "65.524"}
[2025-03-16 08:38:38,532][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 4 @ 12500 updates
[2025-03-16 08:38:38,532][fairseq.trainer][INFO] - Saving checkpoint to checkpoints/checkpoint_4_12500.pt
[2025-03-16 08:38:52,739][fairseq.trainer][INFO] - Finished saving checkpoint to checkpoints/checkpoint_4_12500.pt
[2025-03-16 08:38:58,454][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_4_12500.pt (epoch 4 @ 12500 updates, score 65.103) (writing took 19.922206312417984 seconds)
[2025-03-16 09:12:40,891][train_inner][INFO] - {"epoch": 4, "update": 3.275, "loss": "2.97", "total": "120.455", "n_correct": "60.43", "ppl": "7.84", "accuracy": "50.168", "wps": "4.2", "ups": "0.04", "wpb": "120.5", "bsz": "8", "num_updates": "12600", "lr": "0.000338716", "gnorm": "2.418", "loss_scale": "4096", "train_wall": "4011", "gb_free": "21", "wall": "237695"}
[2025-03-16 09:12:40,891][root][INFO] - Step 12600 | Loss: 2.9700 | Accuracy: 50.17 | LR: 0.000339
[2025-03-16 10:20:15,599][train_inner][INFO] - {"epoch": 4, "update": 3.327, "loss": "3.036", "total": "122.795", "n_correct": "60.605", "ppl": "8.2", "accuracy": "49.355", "wps": "6.1", "ups": "0.05", "wpb": "122.8", "bsz": "8", "num_updates": "12800", "lr": "0.00032872", "gnorm": "2.414", "loss_scale": "4096", "train_wall": "4052", "gb_free": "21", "wall": "241750"}
[2025-03-16 10:20:15,599][root][INFO] - Step 12800 | Loss: 3.0360 | Accuracy: 49.35 | LR: 0.000329
[2025-03-16 11:27:45,660][train_inner][INFO] - {"epoch": 4, "update": 3.379, "loss": "2.92", "total": "122.265", "n_correct": "61.23", "ppl": "7.57", "accuracy": "50.08", "wps": "6", "ups": "0.05", "wpb": "122.3", "bsz": "8", "num_updates": "13000", "lr": "0.000319018", "gnorm": "2.414", "loss_scale": "4096", "train_wall": "4048", "gb_free": "21", "wall": "245800"}
[2025-03-16 11:27:45,660][root][INFO] - Step 13000 | Loss: 2.9200 | Accuracy: 50.08 | LR: 0.000319
[2025-03-16 12:35:36,701][train_inner][INFO] - {"epoch": 4, "update": 3.431, "loss": "2.957", "total": "122.465", "n_correct": "60.98", "ppl": "7.77", "accuracy": "49.794", "wps": "6", "ups": "0.05", "wpb": "122.5", "bsz": "8", "num_updates": "13200", "lr": "0.000309603", "gnorm": "2.502", "loss_scale": "4096", "train_wall": "4069", "gb_free": "21", "wall": "249871"}
[2025-03-16 12:35:36,701][root][INFO] - Step 13200 | Loss: 2.9570 | Accuracy: 49.79 | LR: 0.000310
[2025-03-16 13:43:16,103][train_inner][INFO] - {"epoch": 4, "update": 3.483, "loss": "3.011", "total": "121.855", "n_correct": "59.945", "ppl": "8.06", "accuracy": "49.194", "wps": "6", "ups": "0.05", "wpb": "121.9", "bsz": "8", "num_updates": "13400", "lr": "0.000300466", "gnorm": "2.464", "loss_scale": "4096", "train_wall": "4057", "gb_free": "21", "wall": "253931"}
[2025-03-16 13:43:16,103][root][INFO] - Step 13400 | Loss: 3.0110 | Accuracy: 49.19 | LR: 0.000300