2025-03-13 14:07:28,917 - INFO - This is a test log entry.
2025-03-13 15:10:27,076 - INFO - This is a test log entry.

)
[2025-03-13 15:11:05,870][src.vsp_llm_dataset][INFO] - pad_audio=True, random_crop=False, normalize=True, max_sample_size=500, seqs2seq data=True,
[2025-03-13 15:11:05,870][src.vsp_llm_dataset][INFO] - Noise wav: None->0 wav, Prob: 0.0, SNR: 0, Number of mixture: 1
[2025-03-13 15:11:06,109][fairseq.trainer][INFO] - begin training epoch 1
[2025-03-13 15:11:06,110][fairseq_cli.train][INFO] - Start iterating over samples
[2025-03-13 15:11:06,110][root][INFO] - Start iterating over samples
/home/jupyter-samantha_caasi@dls-bf571/models/VSP-LLM/fairseq/fairseq/tasks/fairseq_task.py:501: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
[2025-03-13 16:13:12,466][train_inner][INFO] - {"epoch": 1, "update": 0.052, "loss": "6.564", "total": "122.985", "n_correct": "28.685", "ppl": "94.65", "accuracy": "23.324", "wps": "6.6", "ups": "0.05", "wpb": "123", "bsz": "8", "num_updates": "200", "lr": "1.49e-05", "gnorm": "11.844", "loss_scale": "128", "train_wall": "3723", "gb_free": "21", "wall": "3727"}
[2025-03-13 16:13:12,467][root][INFO] - Step 200 | Loss: 6.5640 | Accuracy: 23.32 | LR: 0.000015
[2025-03-13 17:14:45,624][train_inner][INFO] - {"epoch": 1, "update": 0.104, "loss": "4.371", "total": "121.88", "n_correct": "41.595", "ppl": "20.69", "accuracy": "34.128", "wps": "6.6", "ups": "0.05", "wpb": "121.9", "bsz": "8", "num_updates": "400", "lr": "2.48e-05", "gnorm": "5.912", "loss_scale": "128", "train_wall": "3690", "gb_free": "21", "wall": "7420"}
[2025-03-13 17:14:45,625][root][INFO] - Step 400 | Loss: 4.3710 | Accuracy: 34.13 | LR: 0.000025
[2025-03-13 18:16:54,503][train_inner][INFO] - {"epoch": 1, "update": 0.156, "loss": "4.185", "total": "122.445", "n_correct": "43.515", "ppl": "18.18", "accuracy": "35.538", "wps": "6.6", "ups": "0.05", "wpb": "122.4", "bsz": "8", "num_updates": "600", "lr": "3.47e-05", "gnorm": "4.042", "loss_scale": "128", "train_wall": "3726", "gb_free": "21", "wall": "11149"}
[2025-03-13 18:16:54,503][root][INFO] - Step 600 | Loss: 4.1850 | Accuracy: 35.54 | LR: 0.000035
[2025-03-13 19:18:54,968][train_inner][INFO] - {"epoch": 1, "update": 0.208, "loss": "4.124", "total": "120.195", "n_correct": "43.615", "ppl": "17.43", "accuracy": "36.287", "wps": "6.5", "ups": "0.05", "wpb": "120.2", "bsz": "8", "num_updates": "800", "lr": "4.46e-05", "gnorm": "3.145", "loss_scale": "128", "train_wall": "3717", "gb_free": "21", "wall": "14869"}
[2025-03-13 19:18:54,968][root][INFO] - Step 800 | Loss: 4.1240 | Accuracy: 36.29 | LR: 0.000045
[2025-03-13 20:21:11,519][train_inner][INFO] - {"epoch": 1, "update": 0.26, "loss": "4.054", "total": "124.19", "n_correct": "45.29", "ppl": "16.61", "accuracy": "36.468", "wps": "6.6", "ups": "0.05", "wpb": "124.2", "bsz": "8", "num_updates": "1000", "lr": "5.45e-05", "gnorm": "2.724", "loss_scale": "128", "train_wall": "3734", "gb_free": "21", "wall": "18606"}
[2025-03-13 20:21:11,520][root][INFO] - Step 1000 | Loss: 4.0540 | Accuracy: 36.47 | LR: 0.000054
[2025-03-13 21:22:53,257][train_inner][INFO] - {"epoch": 1, "update": 0.312, "loss": "4.032", "total": "122.875", "n_correct": "45.255", "ppl": "16.36", "accuracy": "36.83", "wps": "6.6", "ups": "0.05", "wpb": "122.9", "bsz": "8", "num_updates": "1200", "lr": "6.44e-05", "gnorm": "2.318", "loss_scale": "128", "train_wall": "3699", "gb_free": "21", "wall": "22308"}
[2025-03-13 21:22:53,258][root][INFO] - Step 1200 | Loss: 4.0320 | Accuracy: 36.83 | LR: 0.000064
[2025-03-13 22:25:04,928][train_inner][INFO] - {"epoch": 1, "update": 0.364, "loss": "3.998", "total": "124.12", "n_correct": "46.585", "ppl": "15.98", "accuracy": "37.532", "wps": "6.7", "ups": "0.05", "wpb": "124.1", "bsz": "8", "num_updates": "1400", "lr": "7.43e-05", "gnorm": "2.179", "loss_scale": "128", "train_wall": "3729", "gb_free": "21", "wall": "26039"}
[2025-03-13 22:25:04,929][root][INFO] - Step 1400 | Loss: 3.9980 | Accuracy: 37.53 | LR: 0.000074
[2025-03-13 23:27:07,715][train_inner][INFO] - {"epoch": 1, "update": 0.416, "loss": "4.022", "total": "120.275", "n_correct": "44.325", "ppl": "16.24", "accuracy": "36.853", "wps": "6.5", "ups": "0.05", "wpb": "120.3", "bsz": "8", "num_updates": "1600", "lr": "8.42e-05", "gnorm": "1.822", "loss_scale": "128", "train_wall": "3720", "gb_free": "21", "wall": "29762"}
[2025-03-13 23:27:07,716][root][INFO] - Step 1600 | Loss: 4.0220 | Accuracy: 36.85 | LR: 0.000084